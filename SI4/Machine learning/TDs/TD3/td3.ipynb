{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP réseaux de neurones\n",
    "\n",
    "Diane Lingrand (diane.lingrand@univ-cotedazur)\n",
    "\n",
    "Polytech SI4 - CVML - 2020-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports nécessaires pour la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.keras.utils\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "nbClasses = 10 # 10 digits from 0 to 9\n",
    "# flatten the images...\n",
    "xTrain = x_train.reshape(60000, 784)\n",
    "xTest = x_test.reshape(10000, 784)\n",
    "\n",
    "# ... and normalize the data (grey levels are integers from 0 to 255)\n",
    "xTrain = xTrain.astype('float32')/255\n",
    "xTest = xTest.astype('float32')/255\n",
    "\n",
    "# original labels corresponds to digits. We transform the labels to categorical labels.\n",
    "yTrain = tensorflow.keras.utils.to_categorical(y_train, nbClasses)\n",
    "yTest = tensorflow.keras.utils.to_categorical(y_test, nbClasses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape : \",x_train.shape)\n",
    "print(\"y_train shape : \",y_train.shape)\n",
    "print(\"x_test shape : \",x_test.shape)\n",
    "print(\"y_test shape : \",y_test.shape)\n",
    "print(\"xTrain shape : \",xTrain.shape)\n",
    "print(\"xTest shape : \",xTest.shape)\n",
    "print('shape of yTrain :', yTrain.shape)\n",
    "\n",
    "print(type(y_train))\n",
    "print(yTrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=np.random.rand(15,6)\n",
    "t3=np.array([1,4,3,5,8,3,3,9,7,0,2,3,4,5,4])\n",
    "t2=t1[t3==4,:]\n",
    "print(t1)\n",
    "print('\\n')\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case of binary classification\n",
    "\n",
    "# you can change the classes\n",
    "class1 = 4\n",
    "class2 = 8\n",
    "nameClass1 = '4'\n",
    "nameClass2 = '8'\n",
    "\n",
    "## TRAIN\n",
    "#class 1: positives\n",
    "x_train1 = xTrain[y_train==class1,:]\n",
    "#class 2: negatives\n",
    "x_train2 = xTrain[y_train==class2,:]\n",
    "# merging the 2 classes and shuffle\n",
    "x_trainBinaire = np.append(x_train1,x_train2,axis=0)\n",
    "y_trainBinaire = np.append(np.full(len(x_train1),0), np.full(len(x_train2),1))\n",
    "(x_trainBinaire,y_trainBinaire) = shuffle(x_trainBinaire,y_trainBinaire,random_state=0)\n",
    "y_trainBinaire = tensorflow.keras.utils.to_categorical(y_trainBinaire, 2)\n",
    "\n",
    "## TEST\n",
    "#class 1: positives\n",
    "x_test1 = xTest[y_test==class1,:]\n",
    "#class 2: negatives\n",
    "x_test2 = xTest[y_test==class2,:]\n",
    "# merging the 2 classes and shuffle\n",
    "x_testBinaire = np.append(x_test1,x_test2,axis=0)\n",
    "y_testBinaire = np.append(np.full(len(x_test1),0), np.full(len(x_test2),1))\n",
    "(x_testBinaire,y_testBinaire) = shuffle(x_testBinaire,y_testBinaire,random_state=0)\n",
    "y_testBinaire = tensorflow.keras.utils.to_categorical(y_testBinaire, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_trainBinaire)\n",
    "xTrain=xTrain_copy\n",
    "yTrain=yTrain_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Un premier MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClasses=2\n",
    "#Let's build a simple neural network using the keras sequential method\n",
    "model = Sequential()\n",
    "#topology: input as the size of data, one hidden layer with 4 neurons and usual sigmoid activation\n",
    "model.add(Dense(4, input_dim=784, activation='sigmoid'))\n",
    "#model.add(Dense(4, input_dim=784, activation='sigmoid'))\n",
    "#softmax for the output using as many neurons as classes (2 in this case)\n",
    "model.add(Dense(nbClasses, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to define the loss function for the training, the optimisation method (RMSprop) and the accuracy as a metric\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#now, let's train for real the network: only 20 epochs and batch size of 128 (so that an epoch contains 60000/128 iterations)\n",
    "model.fit(x_trainBinaire, y_trainBinaire, epochs=10, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que vaut le score F1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#is it good? we know the truth: y_testBinaire and we will compare to the output of the network\n",
    "\n",
    "score = model.evaluate(x_testBinaire,y_testBinaire)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, f1_score\n",
    "pred_testBinaire = np.argmax(model.predict(x_testBinaire),axis=1)\n",
    "print(pred_testBinaire.shape, y_testBinaire.shape)\n",
    "print(\"F1 score: \", f1_score(pred_testBinaire,np.argmax(y_testBinaire,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mêmes questions avec les 10 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClasses=10 \n",
    "#Let's build a simple neural network using the keras sequential method\n",
    "model = Sequential()\n",
    "#topology: input as the size of data, one hidden layer with 20 neurons and usual sigmoid activation\n",
    "model.add(Dense(20, input_dim=784, activation='softmax'))\n",
    "#model.add(Dense(20, input_dim=784, activation='sigmoid'))\n",
    "\n",
    "#softmax for the output using as many neurons as classes (10 in this case)\n",
    "model.add(Dense(nbClasses, activation='softmax'))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to define the loss function for the training, the optimisation method (RMSprop) and the accuracy as a metric\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#now, let's train for real the network: only 20 epochs and batch size of 128 (so that an epoch contains 60000/128 iterations)\n",
    "model.fit(xTrain, yTrain, epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is it good? we know the truth: y_testBinaire and we will compare to the output of the network\n",
    "\n",
    "score = model.evaluate(xTest,yTest)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "pred_test = np.argmax(model.predict(xTest),axis=1)\n",
    "print(pred_test.shape,np.argmax(yTest,axis=1).shape)\n",
    "print(\"F1 score: \", f1_score(pred_test,np.argmax(yTest,axis=1), average=None))\n",
    "print(\"F1 score micro: \", f1_score(pred_test,np.argmax(yTest,axis=1), average='micro'))\n",
    "print(\"F1 score macro: \", f1_score(pred_test,np.argmax(yTest,axis=1), average='macro'))\n",
    "\n",
    "print('confusion matrix\\n',confusion_matrix(np.argmax(yTest,axis=1), pred_test))\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Essayons de faire mieux ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A t-on laissé le temps à l'algorithme de converger ?\n",
    "Modifiez le nombre d'itérations. Les résultats sont-ils meilleurs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critère d'arrêt autre que le nombre d'itérations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small example, we decided, as a default behavior, to stop after 20 epochs. Of course this value can be changed. Another way to deal with that is to use early stopping criterion. All options are described in the keras documentation. Feel free to experiment all options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# we define a callback function that will control if the accuracy \n",
    "# on the validation set (a part of train set) is not changing more than 10-4 with a patience of 20 iterations\n",
    "# If the last accuracy value is not the best one, we still keep the last results\n",
    "# In this example, we extracted 20% of the train set for the validation set that will be used to monitor the convergence.\n",
    "\n",
    "ourCallback = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "# let's learn the network again !\n",
    "# We do not know when the training will stop but no more than 2000 epochs.\n",
    "model.fit(xTrain, yTrain, epochs=2000, batch_size=128, validation_split=0.2, callbacks=[ourCallback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quel 'epoch' l'algorithme s'est-il arrêté ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution de la convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('model accuracy')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "ourCallback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "logdir = os.path.join(\"/home/safwane/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboardCb = tensorflow.keras.callbacks.TensorBoard(logdir) #, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "historyCNN = model.fit(xTrain, yTrain, epochs=2000, batch_size=2048, validation_split=0.2, callbacks=[tensorboardCb,ourCallback])\n",
    "plot_history(historyCNN)\n",
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La topologie du réseau convient ?\n",
    "Ajoutez des neurones à la couche cachée ou bien augmentez le nombre de couches cachées.\n",
    "Exemple avec 2 couches cachées de 20 neurones (utilisez les lignes en les modifiant et en enlevant les commentaires):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Dense(20, input_dim=784, activation='sigmoid'))\n",
    "#model.add(Dense(20, activation='sigmoid'))\n",
    "#model.add(Dense(nbClasses, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce qu'augmenter le nombre de couches de neurones augmente les performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce qu'augmenter le nombre de neurones par couche augmente les performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifiez également l'activation 'sigmoid' par 'relu'. Observez-vous une différence ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essayez les différents 'optimizer' disponibles. Quels sont-ils ? Observez-vous des différences ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modifiez les données en considérant FMNIST (Fashion MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train6 = x_train[y_train==6,:]\n",
    "x_train0 = x_train[y_train==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape : \",x_train.shape)\n",
    "print(\"y_train shape : \",y_train.shape)\n",
    "print(\"x_test shape : \",x_test.shape)\n",
    "print(\"y_test shape : \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 10)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "nbClasses = 10 # 10 digits from 0 to 9\n",
    "# flatten the images...\n",
    "# ... and normalize the data (grey levels are integers from 0 to 255)\n",
    "xTrain = xTrain.astype('float32')/255\n",
    "xTest = xTest.astype('float32')/255\n",
    "\n",
    "# original labels corresponds to digits. We transform the labels to categorical labels.\n",
    "yTrain = tensorflow.keras.utils.to_categorical(y_train, nbClasses)\n",
    "yTest = tensorflow.keras.utils.to_categorical(y_test, nbClasses)\n",
    "print(yTrain.shape)\n",
    "yTrain.reshape(60000,-1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    image=x_train6[i]\n",
    "    plt.imshow(image,cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=[0,1,2,4,5,6]\n",
    "t2=tensorflow.keras.utils.to_categorical(t1)\n",
    "print(t2)\n",
    "print(np.argmax(t2,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    image=x_train[i]\n",
    "    plt.imshow(image,cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClasses=10 \n",
    "#Let's build a simple neural network using the keras sequential method\n",
    "model = Sequential()\n",
    "#topology: input as the size of data, one hidden layer with 20 neurons and usual sigmoid activation\n",
    "model.add(Dense(20, input_dim=784, activation='sigmoid'))\n",
    "model.add(Dense(20, input_dim=784, activation='sigmoid'))\n",
    "#model.add(Dense(20, input_dim=784, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#softmax for the output using as many neurons as classes (10 in this case)\n",
    "model.add(Dense(nbClasses, activation='sigmoid'))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.7347 - accuracy: 0.5416\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0337 - accuracy: 0.6739\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7162 - accuracy: 0.7821\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.5609 - accuracy: 0.8251\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.4823 - accuracy: 0.8428\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.4411 - accuracy: 0.8514\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.4160 - accuracy: 0.8579\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3994 - accuracy: 0.8626\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3867 - accuracy: 0.8662\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3757 - accuracy: 0.8684\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3674 - accuracy: 0.8711\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3600 - accuracy: 0.8737\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3532 - accuracy: 0.8756\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3472 - accuracy: 0.8779\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3423 - accuracy: 0.8802\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3374 - accuracy: 0.8812\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3329 - accuracy: 0.8832\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3294 - accuracy: 0.8836\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3256 - accuracy: 0.8853\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3222 - accuracy: 0.8860\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3193 - accuracy: 0.8874\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3163 - accuracy: 0.8878\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3135 - accuracy: 0.8884\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.3111 - accuracy: 0.8894\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.3087 - accuracy: 0.8908\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3062 - accuracy: 0.8915\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.3047 - accuracy: 0.8921\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3016 - accuracy: 0.8927\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.3001 - accuracy: 0.8938\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2979 - accuracy: 0.8938\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2962 - accuracy: 0.8949\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2948 - accuracy: 0.8953\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2926 - accuracy: 0.8959\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2914 - accuracy: 0.8963\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2896 - accuracy: 0.8977\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2876 - accuracy: 0.8981\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2864 - accuracy: 0.8979\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2853 - accuracy: 0.8985\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2840 - accuracy: 0.8992\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2818 - accuracy: 0.8999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d0c33b828>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to define the loss function for the training, the optimisation method (RMSprop) and the accuracy as a metric\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#now, let's train for real the network: only 20 epochs and batch size of 128 (so that an epoch contains 60000/128 iterations)\n",
    "model.fit(xTrain, yTrain, epochs=40, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.3740 - accuracy: 0.8690\n",
      "accuracy: 86.90%\n",
      "(10000,) (10000,)\n",
      "F1 score:  [0.81945838 0.96911392 0.78347437 0.86752988 0.77373737 0.94944388\n",
      " 0.67683227 0.93631841 0.96586345 0.95200396]\n",
      "F1 score micro:  0.869\n",
      "F1 score macro:  0.8693775889253903\n",
      "confusion matrix\n",
      " [[817   3  12  32   3   1 122   0   9   1]\n",
      " [  2 957   2  30   5   1   1   0   2   0]\n",
      " [ 17   1 787  13  91   1  89   0   1   0]\n",
      " [ 24  11   8 871  51   0  31   0   4   0]\n",
      " [  0   1 117  27 766   0  87   0   2   0]\n",
      " [  0   0   0   1   0 939   0  35   1  24]\n",
      " [131   1  80  29  60   0 688   0  11   0]\n",
      " [  0   0   0   0   0  25   0 941   0  34]\n",
      " [  3   1   3   5   4   4  14   4 962   0]\n",
      " [  0   0   0   0   0   7   1  30   0 962]]\n"
     ]
    }
   ],
   "source": [
    "#is it good? we know the truth: y_testBinaire and we will compare to the output of the network\n",
    "\n",
    "score = model.evaluate(xTest,yTest)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "pred_test = np.argmax(model.predict(xTest),axis=1)\n",
    "print(pred_test.shape,np.argmax(yTest,axis=1).shape)\n",
    "print(\"F1 score: \", f1_score(pred_test,np.argmax(yTest,axis=1), average=None))\n",
    "print(\"F1 score micro: \", f1_score(pred_test,np.argmax(yTest,axis=1), average='micro'))\n",
    "print(\"F1 score macro: \", f1_score(pred_test,np.argmax(yTest,axis=1), average='macro'))\n",
    "\n",
    "print('confusion matrix\\n',confusion_matrix(np.argmax(yTest,axis=1), pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "learned\n"
     ]
    }
   ],
   "source": [
    "## apprentissage par boosting (Adaboost)\n",
    "# création de l'object boosting\n",
    "myboosting = ensemble.AdaBoostClassifier(n_estimators=100, learning_rate=0.1, algorithm='SAMME.R')\n",
    "# apprentissage sur les données 'train'\n",
    "print(xTrain.shape)\n",
    "print(yTrain.shape)\n",
    "myboosting.fit(xTrain, y_train)\n",
    "print(\"learned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.31206371797602106\n"
     ]
    }
   ],
   "source": [
    "pred_test = (myboosting.predict(xTest))\n",
    "\n",
    "print(\"F1 score : \", f1_score(pred_test,y_test,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.5357\n"
     ]
    }
   ],
   "source": [
    "pred_test = (myboosting.predict(xTest))\n",
    "\n",
    "print(\"F1 score : \", f1_score(pred_test,y_test,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/safwane/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#default constructor\n",
    "myRegLog = LogisticRegression()\n",
    "#myRegLog = LogisticRegression(penalty=\"elasticnet\",solver=\"saga\",l1_ratio=0.5)\n",
    "#learning\n",
    "myRegLog.fit(xTrain,y_train)\n",
    "print(\"learning done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.8438\n"
     ]
    }
   ],
   "source": [
    "pred_test = myRegLog.predict(xTest)\n",
    "print(\"F1 score : \", f1_score(pred_test,y_test,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: nan - accuracy: 0.1080\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: nan - accuracy: 0.1000\n",
      "10000/10000 [==============================] - 0s 19us/sample - loss: nan - accuracy: 0.1000\n",
      "accuracy: 10.00%\n",
      "(10000,) (10000,)\n",
      "F1 score:  [0.18181818 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "F1 score micro:  0.10000000000000002\n",
      "F1 score macro:  0.01818181818181818\n",
      "confusion matrix\n",
      " [[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation \n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "import tensorflow.keras.utils\n",
    "\n",
    "\n",
    "nbClasses = 10 # 10 digits from 0 to 9\n",
    "# flatten the images...\n",
    "xTrain = x_train.reshape(60000, 784)\n",
    "xTest = x_test.reshape(10000, 784)\n",
    "# ... and normalize the data (grey levels are integers from 0 to 255)\n",
    "xTrain = xTrain.astype('float32')/255\n",
    "xTest = xTest.astype('float32')/255\n",
    "\n",
    "yTrain = tensorflow.keras.utils.to_categorical(y_train, nbClasses)\n",
    "yTest = tensorflow.keras.utils.to_categorical(y_test, nbClasses)\n",
    "\n",
    "#Let's build a simple neural network using the keras sequential method\n",
    "model = Sequential()\n",
    "#topology: input as the size of data, one hidden layer with 20 neurons and usual sigmoid activation\n",
    "model.add(Dense(20, input_dim=784, activation='softmax'))\n",
    "model.add(Dense(20, input_dim=784, activation='softmax'))\n",
    "model.add(Dense(20, input_dim=784, activation='softmax'))\n",
    "model.add(Dense(20, input_dim=784, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sigmoid for the output using as many neurons as classes (10 in this case)\n",
    "model.add(Dense(nbClasses, activation='relu'))\n",
    "\n",
    "#we need to define the loss function for the training, the optimisation method (RMSprop) and the accuracy as a metric\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#now, let's train for real the network: only 20 epochs and batch size of 128 (so that an epoch contains 60000/128 iterations)\n",
    "model.fit(xTrain, yTrain, epochs=20, batch_size=128)\n",
    "\n",
    "\n",
    "score = model.evaluate(xTest,yTest)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "pred_test = np.argmax(model.predict(xTest),axis=1)\n",
    "print(pred_test.shape,np.argmax(yTest,axis=1).shape)\n",
    "print(\"F1 score: \", f1_score(pred_test,np.argmax(yTest,axis=1), average=None))\n",
    "print(\"F1 score micro: \", f1_score(pred_test,np.argmax(yTest,axis=1), average='micro'))\n",
    "print(\"F1 score macro: \", f1_score(pred_test,np.argmax(yTest,axis=1), average='macro'))\n",
    "\n",
    "print('confusion matrix\\n',confusion_matrix(np.argmax(yTest,axis=1), pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
